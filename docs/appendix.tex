% !TeX root = master.tex

\section{Appendix}

This appendix collects small implementation notes that are useful for understanding the
code but are not central to the main method descriptions.

\subsection{Coordinate system: latitude/longitude to meters}

This subsection explains the local projection used so the filters can operate in meters.

Internally, the filter runs in a local tangent-plane approximation. We choose a local
origin near the current area and convert \((\text{lat}, \text{lon})\) into \((x, y)\)
meters relative to that origin.

This is implemented with helpers in \texttt{geo.js} (\texttt{toMeters}, \texttt{fromMeters}).
Over the distances we care about at a start line, this approximation is accurate enough and
avoids complicated geodesy inside the filter.

\subsection{Diagnostics recording pipeline}

This subsection explains how the app captures raw sensor streams for diagnostics without
interfering with the on-water workflow.

Recording is user-initiated and begins with a short note so a session can be identified
later. Once recording starts, the app immediately switches GPS to high accuracy for the
duration of the session, because both the Kalman filter and later analysis depend on the
uncertainty fields that are only meaningful at that setting. The recorder does not attempt
to synchronize sensors into frames; instead, each GPS or IMU event is stored with its own
timestamps. For GPS this includes both the GPS-reported time and the device wall clock,
which is necessary for later alignment to higher-precision RTK data. For IMU data, the
raw rotation rates, acceleration, and acceleration including gravity are stored alongside
the mapping used for yaw extraction, so the reconstruction path can be audited.

Storage is implemented in \texttt{core/recording.js} using an IndexedDB store named
\texttt{racetimer-recording}. Each record includes a device ID, a session ID, a timestamp
used for ordering, and a typed payload. An in-memory queue batches records into NDJSON
chunks (roughly half a megabyte or a couple of seconds of samples), which are then written
as chunk objects in IndexedDB. These chunks form a small upload queue that is capped by
total size (about 5\,MB by default), so the app never accumulates unbounded local storage
even if the network is down. When the queue is full, the oldest data chunks are dropped,
which makes the system resilient while still preserving the most recent diagnostics.

\subsection{Record types and upload format}

This subsection describes what is written to disk and how those records are packaged for
upload.

At session start a \texttt{meta} record stores the user note, settings snapshot, device
information, and build stamp. As the session runs, the app writes \texttt{gps} records
containing the full coordinate payload plus both GPS and device time, and \texttt{imu}
records containing the raw motion samples. The recorder intentionally omits Kalman and
derived state so playback can run the same filter code as live operation, which keeps the
recording closer to the raw sensor truth and avoids baking in older filter behavior.

Instead of a manual send step, the app streams chunks as they are produced. Each chunk is
posted with headers that identify the device, session, chunk index, and time range, and the
payload remains line-oriented NDJSON so each record is independent. The worker stores each
chunk under a device/session prefix and maintains a manifest file that lists the chunks in
order, which is enough to reconstruct a full session on demand without requiring an append
operation on the storage backend. If an upload key is configured, the app includes it as a
bearer token so only authorized clients can post data.

\subsection{Wind data pipeline and history}

This subsection explains how the wind service collects samples, deduplicates them, and
serves a 24-hour history without requiring active clients.

Wind data is sourced from the Weather Display \texttt{clientraw.txt} feed and processed by
a dedicated worker. Each fetch parses wind speed, gust, and direction, and also extracts
the station time fields so the response can report both the worker timestamp and the source
clock. The raw payload is hashed before parsing so the worker can recognize repeated samples
and avoid appending identical points to history. The latest sample and a rolling 24-hour
array are stored in a KV namespace under stable keys, and the response includes the
\texttt{sampleHash} and an \texttt{ageSeconds} field so clients can see when the last change
arrived.

Because cron triggers only run once per minute, a Durable Object alarm is used to maintain
15\,s sampling. The sampler schedules its own alarm every 15\,s and calls the same history
update routine used by the on-demand fetch path, so the history remains dense even when no
clients are active. A one-minute cron trigger is still configured as a safety net and also
boots the sampler if it has been idle. The public endpoint remains \texttt{/wind}, which
supports optional query flags such as \texttt{history=1} and \texttt{hours=24} for returning
the stored history, or \texttt{refresh=1} to force an immediate upstream pull.

\subsection{RaceWind windowing and cache behavior}

This subsection summarizes the client-side performance tricks used in the RaceWind view so
large history windows remain responsive on mobile hardware.

The RaceWind client retains only a bounded horizon locally (currently eight hours) even
though the server can provide more. The trim happens on ingest, so older samples never
enter the in-memory ring. When the window size changes, the client does not discard the
history and refetch; instead it merges any returned history into the existing sample list
and deduplicates by timestamp, which keeps the timeline stable and avoids extra churn.

Two caches keep redraws lightweight. The history slice shown on the plots is obtained by
binary-searching into the timestamped sample array, rather than scanning the entire list.
That slice is cached for the current horizon so the speed and direction plots reuse the
same window without repeating the search. For the spectral analysis, the periodogram
selection and joint fit are cached per history window and period cap. As long as the
underlying samples are unchanged, the reconstruction overlay reuses that cached selection
and fit rather than recomputing the spectrum on every paint.

Finally, history fetches are queued. If a live sample fetch is already in flight when the
user expands the horizon, the requested history load is recorded and executed as soon as
the network becomes idle. This avoids dropped requests while keeping the code path simple
and deterministic under real-time polling.

\subsection{Replay pipeline and timing model}

This subsection explains how recorded sessions are fed back into the app so algorithms
see the same inputs as they would on the water.

Replay uses a manifest file (\texttt{replay/manifest.json}) to list locally available data
files. When a replay session starts, the app stops real GPS/IMU listeners, clears state,
and feeds the raw GPS and IMU records back through the same input handlers used on the
water. Older recordings may still include \texttt{derived} samples, and the replay path
keeps compatibility with those files, but new recordings rely on raw inputs only. Each
record carries a device-time offset from the start of the file, and optional sensor times
(GPS time or motion-event time) are converted into matching offsets so the original relative
timing is preserved.

During playback, the app advances a virtual replay clock and dispatches each record through
the same entry points used by live sensors (\texttt{handlePosition} for GPS and the IMU
processing pipeline for motion). This means the Kalman filter, heading logic, and plots
cannot distinguish replay from live data. The replay speed simply scales the virtual clock,
and time-based components read that clock rather than the system wall clock, ensuring that
filter time steps remain consistent even at higher playback speeds. To keep the UI responsive,
the replay loop processes events in bounded batches per tick.
